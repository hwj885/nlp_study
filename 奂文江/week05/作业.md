
客服工作台 作业
https://help.aliyun.com/zh/document_detail/421160.html?spm=a2c4g.11186623.0.0.30f638fe8lQYuY
功能1: faq 类目管理，一级类目，二级类目， 对应的faq 信息
https://help.aliyun.com/zh/document_detail/421161.html?spm=a2c4g.11186623.0.0.30f67558YDiTwC
功能2: faq录入和管理，待选提问、相似提问、生效时间、对应回答

使用场景：代替人工客服，用户的提问直接与历史的提问进行相似度匹配，最相似的提问的回答返回给用户。
作业1（400字文档）: 阅读上面的客服工作台的说明，总结我们自己作为后端开发和算法开发的角色，我们需要做什么？
    ◦ 需要设计数据库吗？
    ◦ 需要使用什么模型？
    ◦ 如何使用使用bert的？
    ◦ 是否需要使用大模型？
1.我们需要设计数据库用于存储用户添加的FAQ数据。除了数据的持久化，我们还需考虑检索效率，可以通过缓存来提升响应速度。
2.首先选用bert预训练模型，我们将收集到的FAQ数据进行bert模型的微调，bert的响应速度快，准确率高，成本低廉，能够满足对话需求。
3.具体可以使用sentence-bert，可以将用户的提问与FAQ的问题进行相似度匹配，再根据匹配结果可以快速检索到问题的答案。
4.肯定需要，一些除了FAQ的问题，需要大模型来回答，比如多轮对话，复杂问题的生成，当然使用大模型也存在成本高昂，响应速度慢等问题。
5.sentence-bert对中文的支持一般，为了更好的支持中文，我们选择对中文支持性更好的BGE (BAAI/bge-base-zh)。


作业2（400字文档， 流程图）: 如何使用bert 进行文本编码，并且使用bert 进行相似度计算，需要写清楚技术方案；

1. 在对句子进行文本编码时，我们选用池化技术mean-pooling，BERT的输出为last_hidden_state（所有tokens的隐藏状态，形状为[batch_size, seq_len, hidden_size]），需将其转换为句子级向量。常用方法：
[CLS]token：取last_hidden_state[:, 0, :]（[CLS]是BERT用于分类的特殊token，其隐藏状态蕴含句子整体语义）， 均值池化，对所有有效tokens（attention_mask为1）的隐藏状态取平均（mean(dim=1)），更适用于长文本。
2. 向量归一化将向量缩放至单位长度（L2范数为1），确保余弦相似度计算的准确性（余弦相似度对向量长度敏感）。
3. 余弦相似度（Cosine Similarity）算法进行计算，衡量两个文本向量的语义相似度，常用余弦相似度（取值范围[-1, 1]，值越大表示越相似）。


